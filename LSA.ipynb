{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA : Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* ETC\n",
    "  - [Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame( {'doc': documents } )\n",
    "\n",
    "# remain only alphabets\n",
    "news_df['clean_doc'] = news_df['doc'].str.replace( \"[^a-zA-Z#]\", \" \" )\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply( lambda x: ' '.join( [ w for w in x.split() if len(w) > 3 ] ) )\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply( lambda x: x.lower() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom nltk.corpus import stopwords\\nstop_words = stopwords.words('english')\\n\\n# tokenization\\ntokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\\n\\n# remove stop-words\\ntokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\\n\\n# de-tokenization\\ndetokenized_doc = []\\nfor i in range(len(news_df)):\\n    t = ' '.join(tokenized_doc[i])\\n    detokenized_doc.append(t)\\n\\nnews_df['clean_doc'] = detokenized_doc\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\"\"\"\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer( stop_words='english', \n",
    "                             max_features=1000, # kepp top 1000 terms\n",
    "                             max_df = 0.05, smooth_idf=True )\n",
    "\n",
    "X = vectorizer.fit_transform( news_df['clean_doc'] )\n",
    "print( X.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ability absolutely accept access according account action actions acts actual added addition additional address administration advance advice agencies agree algorithm allow allowed allows amendment america american americans analysis angeles anonymous answer answers anti anybody apparently appear appears apple application applications apply appreciate appreciated approach appropriate april arab archive area areas aren argument arguments armenia armenian armenians arms army article articles asked asking assume assuming atheism atheists attack attacks attempt author authority auto average avoid aware away background base baseball based basic basically basis begin beginning belief beliefs bible bike bios bits black block blood blue board body book books boston bought break bring brought build building built business cable california calling calls came canada card cards care carry cars cases cause caused center certain certainly chance change changed changes char character cheap check chicago child children chip chips choice choose christ christian christianity christians church citizens city civil claim claims class clear clearly client clinton clipper clock close code color colorado colors comes coming command comment comments commercial committee common communications community comp companies company compatible complete completely computer concerned conclusion condition conference congress connection consider considered considering constitution contact contains context continue contrib control controller convert copies copy correct cost costs couldn count countries country couple court cover create created crime criminal criminals cross cryptography current currently data date dave david days dead deal death decided decision default defense define definition deleted department described description design designed details detroit developed development device devices died difference difficult digital direct directly directory discussion disease disk disks display distribution division doctor doing door double doubt draw drive driver drivers drives driving drug drugs earlier early earth easily east easy education effect effective effort electronic email encryption energy enforcement engine entire entirely entries entry environment equipment error errors escrow especially europe event events evidence evil exactly example excellent exist existence existing exists expect expected expensive experience explain export external extra face fair fairly faith fall false family fast faster father features federal feel field figure file files final finally fine firearms floppy folks follow font fonts food force forget form format forward free freedom friend friends fully function functions future game games gave general generally genocide george german germany gets getting given gives giving goal goals goes gone government graphics greatly greek ground group groups guess guide guns guys half hand handle hands happen happened happens happy hardware haven head health hear heard heaven held hell hello higher highly history hockey hold holy home hope host hours house human idea ideas image images imagine important impossible include included includes including increase independent individual industry info input inside installed instead institute insurance intended interested interesting interface internal international internet involved islam israel israeli issue issues james jesus jewish jews jobs john jpeg judge justice kept keyboard keys kill killed killing kind king knew knowledge known knows koresh lack land language large late later latest launch laws lead league learn leave left legal letter level library license life light likely limited line lines list live lived lives living local logic longer looked looks lord lost lots love lower luck lunar machine machines magazine mailing main major majority makes making management manager manual march mark market mass master material matter matthew mean meaning means meant media medical member members memory mention mentioned message messages method michael middle mike miles military million mind minutes misc missing mission mode model models modem modern money monitor month months moon moral morning mother motherboard motif mouse multi multiple muslim muslims names nasa national natural nature nazi near necessary needed needs network news newsgroup nice night normal north note notice nuclear null numbers object objective obvious obviously offer offers office official ones open operation opinion opinions option options orbit order organization original output outside package page paid pain paper particular particularly parts party pass passed past patients paul peace perfect performance period person personal personally peter phone physical pick picture pittsburgh place places plan play played player players playing plus points police policy political poor population port position possibly posted posting posts postscript practice present president press pressure pretty prevent previous price print printer privacy private problems process processing produce product products program programming programs project proof properly property protect protection prove provide provided provides public published purpose quality questions quite quote radio random range rate rates reading reality realize reason reasonable reasons recall receive received recent recently record reference references regarding regular related release religion religious remember remote reply report reported reports request require required requires research resource resources respect response responsible rest result results return rights risk road robert room round rule rules running runs russia russian safe safety sale satellite save saying says school science scientific screen scsi search season secret section secure security seen self sell send sense sent separate serial series seriously server service services setting shall share shipping short shot shouldn shows shuttle signal similar simple simply single site sites situation size slow small smith social society software sold soldiers solution somebody soon sorry sort sound sounds source sources south soviet space speak special specific specifically speed spent spirit stand standard standards start started starting state stated statement states station stay step stephanopoulos steve stop story stream street string strong studies study stuff stupid subject suggest suggestions summer supply support supported supports suppose supposed surface suspect switch systems table taken takes taking talk talking tape team teams technical technology telephone term terms test text thank theory thinking thought times title today told took tools toronto total totally town trade traffic training transfer tried trouble trust truth trying turkey turkish turks turn turned type types understand understanding unfortunately unit united universe university unix unless useful usenet user users uses usually valid value values vancouver various vehicle version versions video view voice volume wait wanted wants washington wasn watch water ways weapon weapons week weeks went west western white wide widget wife willing window wire wish woman women wonder wondering word words worked working works worse worth wouldn write writing written wrong wrote xterm yeah york young\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print( \" \".join( terms ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling via SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_model = TruncatedSVD( n_components=20, algorithm=\"randomized\", n_iter=100, random_state=122 )\n",
    "svd_model.fit( X )\n",
    "\n",
    "# The components of svd_model are our topics\n",
    "print( svd_model.components_.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Topic 01 : drive card program file government\n",
      "# Topic 02 : drive card scsi disk file\n",
      "# Topic 03 : game team games season players\n",
      "# Topic 04 : drive scsi drives jesus controller\n",
      "# Topic 05 : card video monitor drivers cards\n",
      "# Topic 06 : chip government clipper encryption phone\n",
      "# Topic 07 : email sale offer interested condition\n",
      "# Topic 08 : bike window engine speed space\n",
      "# Topic 09 : israel space card israeli file\n",
      "# Topic 10 : space nasa data shuttle launch\n",
      "# Topic 11 : window israel israeli display server\n",
      "# Topic 12 : address advance email info list\n",
      "# Topic 13 : game bike israel space chip\n",
      "# Topic 14 : israel team bike israeli jews\n",
      "# Topic 15 : window file list space bike\n",
      "# Topic 16 : program files government anybody armenian\n",
      "# Topic 17 : list bike software game version\n",
      "# Topic 18 : armenian armenians version turkish software\n",
      "# Topic 19 : monitor software color computer info\n",
      "# Topic 20 : heard list games program chip\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate( svd_model.components_ ):\n",
    "    terms_comp = zip( terms, comp )   # term and it's score of each topics\n",
    "    sorted_terms = sorted( terms_comp, key=lambda x: x[1], reverse=True )[:5]\n",
    "    top5 = [  term for term, score in  sorted_terms ] \n",
    "    print( \"# Topic {:02d} : {}\".format( i+1, \" \".join( top5 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20)\n",
      "[ 0.10576955 -0.08813723 -0.04810648  0.04913651  0.01133246  0.01591086\n",
      " -0.05865154 -0.00723201  0.13446685 -0.0818737   0.06789282  0.00350589\n",
      "  0.04187729  0.06997614 -0.01820805 -0.02899951  0.01215114  0.00526533\n",
      " -0.00040477  0.02610957]\n"
     ]
    }
   ],
   "source": [
    "X_topics = svd_model.transform(X)\n",
    "print( X_topics.shape )\n",
    "print( X_topics[0, : ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'umap' has no attribute 'UMAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-167aeed6786e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUMAP\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'umap' has no attribute 'UMAP'"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge umap-learn\n",
    "import umap\n",
    "\n",
    "embedding = umap.UMAP( n_neighbors=150, min_dist=0.5, random_state=12 ).fit_transform(X_topics)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "c = dataset.target,\n",
    "s = 10, # size\n",
    "edgecolor='none'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

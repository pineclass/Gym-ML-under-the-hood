{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA : Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* ratsgo'blog\n",
    "  - [Topic Modeling, LDA](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/06/01/LDA/)\n",
    "  - [Topic Modeling, LDA 구현](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/09/lda/)\n",
    "  - [밑바닥부터 시작하는 데이터 사이언스 / 예시코드](https://github.com/Insight-book/data-science-from-scratch/blob/master/code-python3/natural_language_processing.py)\n",
    "  \n",
    "* etc\n",
    "  - [Topic Modeling with Scikit Learn](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730)\n",
    "  - [Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "DT  = DP dot PT\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__DT : Documents-Term matrix__\n",
    "\n",
    "| DT | t1 | t2 | t3 | t4 |\n",
    "|----|----|----|----|----|\n",
    "| D1 | 0  | 0  | 0  | 0  |\n",
    "| D2 | 0  | 0  | 0  | 0  |\n",
    "| D3 | 0  | 0  | 0  | 0  |\n",
    "\n",
    "\n",
    "\n",
    "__DP : Documents-Topic matrix__\n",
    "\n",
    "\n",
    "| DP | Topic1 | Topic2 |\n",
    "|----|--------|--------|\n",
    "| D1 |   0     |   0     |\n",
    "| D2 |   0     |   0     |\n",
    "| D3 |   0     |   0     |\n",
    "\n",
    "\n",
    "__PT : Topic-term matrix__\n",
    "\n",
    "|  PT    | t1 | t2 | t3 | t4 |\n",
    "|:------:|:--:|:--:|:--:|:--:|\n",
    "| Topic1 |  0  | 0   | 0   |   0 |\n",
    "| Topic2 | 0   |  0  |  0  | 0   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data[:200]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame( {'document': documents } )\n",
    "\n",
    "# remain only alphabets\n",
    "news_df['clean_doc'] = news_df['document'].str.replace( \"[^a-zA-Z#]\", \" \" )\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply( lambda x: ' '.join( [ w for w in x.split() if len(w) > 3 ] ) )\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply( lambda x: x.lower() )\n",
    "# make words list\n",
    "news_df['list_doc'] = news_df['clean_doc'].apply( lambda x: x.split() )\n",
    "# make doc size\n",
    "news_df['doc_size'] = news_df['list_doc'].apply( lambda x: len(x) )\n",
    "# make unique words size\n",
    "news_df['doc_uniq_words_size'] = news_df['list_doc'].apply( lambda x: len( set(x) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variables and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 총 문서\n",
    "doc = news_df['list_doc']\n",
    "# 총 문서 수 \n",
    "D = len( doc )\n",
    "# 총 토픽 수\n",
    "K = number_of_topics = 20\n",
    "\n",
    "# 문서마다 각 토픽에 해당하는 단어의 수. shape=( D, K )\n",
    "doc_topic_counts = [ Counter() for _ in  doc ]\n",
    "# 각 단어가 각 토픽에 할당되는 횟수. shape=( K, words_dic )\n",
    "topic_word_counts = [ Counter() for _ in range(K) ]\n",
    "# 각 토픽에 할당되는 총 단어수\n",
    "topic_counts = [ 0 for _ in range(K) ]\n",
    "# 각 문서에 포함되는 총 단어수\n",
    "doc_lengths = news_df['doc_size']\n",
    "# 단어의 종류수\n",
    "news_df['doc_uniq_words_size']\n",
    "voca = set( sum( doc, [] ) )\n",
    "V = len( voca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document( topic, d, alpha=0.1 ):\n",
    "    \"\"\"\n",
    "    문서 d의 모든 단어 가운데 topic에 속하는 단어의 비율 ( with add-alpha smoothing )\n",
    "    \"\"\"\n",
    "    return ( doc_topic_counts[d][topic] + alpha ) / ( news_df['doc_size'][d] + K * alpha )\n",
    "\n",
    "def p_word_given_topic( word, topic, beta=0.1 ):\n",
    "    \"\"\"\n",
    "    토픽에 속한 단어 가운데 word의 비율\n",
    "    \"\"\"\n",
    "    return ( ( topic_word_counts[topic][word] + beta ) ) / ( topic_counts[topic] + V * beta )\n",
    "\n",
    "def topic_weight( d, word, k ):\n",
    "    \"\"\"\n",
    "    문서와 문서의 단어가 주어지면 k번째 토픽의 weight를 반환\n",
    "    \"\"\"\n",
    "    return p_word_given_topic( word, k ) * p_topic_given_document( k, d )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_topic( d, word ):\n",
    "    return sample_from( [ topic_weight(d, word, k) for k in range(K) ]  )\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "def sample_from( weights ):\n",
    "    \"\"\"\n",
    "    i를 weights[i] / sum(weights) 확률로 반환\n",
    "    아래 식을 만족하는 가장 작은 i를 반환\n",
    "    weights[0] + ... + weights[i] >= rnd\n",
    "    \"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()      # 0과 total 사이를 균일하게 선택\n",
    "    for i, w in enumerate( weights ):\n",
    "        rnd -= w\n",
    "        if rnd <= 0 : return i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling via LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "doc_topics = [ [ random.randrange( K ) for word in d ] for d in doc ]\n",
    "\n",
    "# AB를 구하는데 필요한 숫자를 셈\n",
    "for d in range( D ):\n",
    "    for word, topic in zip( doc[d], doc_topics[d] ):\n",
    "        doc_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Junho\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Junho\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3be188190154>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mtopic_word_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_topic\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtopic_counts\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mnew_topic\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mdoc_lengths\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m         \u001b[1;31m# do the setitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m         \u001b[0mcacher_needs_updating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_chained_assignment_possible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1039\u001b[0m         \u001b[0msetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcacher_needs_updating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mref\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3188\u001b[0m                 self._check_setitem_copy(stacklevel=4, t='referant',\n\u001b[1;32m-> 3189\u001b[1;33m                                          force=True)\n\u001b[0m\u001b[0;32m   3190\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_check_setitem_copy\u001b[1;34m(self, stacklevel, t, force)\u001b[0m\n\u001b[0;32m   3234\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3235\u001b[0m                 \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3236\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_referents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3237\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3238\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 깁스 셈플링\n",
    "\n",
    "times = 5\n",
    "\n",
    "for iter in range( times ):\n",
    "    for d in range( D ):\n",
    "        for i, (word, topic) in enumerate( zip( doc[d], doc_topics[d] ) ):\n",
    "            # 샘플링 대상 word와 topic을 제외하고 세어봄\n",
    "            doc_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[ topic ] -= 1\n",
    "            doc_lengths[ d ] -= 1\n",
    "            \n",
    "            # 깁스 샘플링 대상 word와 topic을 제외한\n",
    "            # 말뭉치 모든 word의 topic 정보를 토대로\n",
    "            # 샘플링 대상 word의 새로운 topic을 선택\n",
    "            new_topic = choose_new_topic( d, word )\n",
    "            doc_topics[d][i] = new_topic\n",
    "            \n",
    "            # 샘프링 대상 word의 새로운 topic을 반영해 \n",
    "            # 말뭉치 정보 업데이트\n",
    "            doc_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[ new_topic ] += 1\n",
    "            doc_lengths[ d ] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_features = 1000\n",
    "no_topics = 20\n",
    "no_top_words = 10\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation( n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print( \"Topic %d:\" % (topic_idx) )\n",
    "        print(\" \".join([feature_names[i] \n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]) )\n",
    "        \n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
